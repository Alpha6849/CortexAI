{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57a47f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using cleaned dataset: ../data/cleaned\\cleaned_dataset_20251127_225256.csv\n",
      "Shape: (150, 5)\n",
      " Target column detected: Species\n",
      " Task type detected: classification\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cleaned_folder = \"../data/cleaned\"\n",
    "cleaned_files = glob.glob(os.path.join(cleaned_folder, \"*.csv\"))\n",
    "\n",
    "if not cleaned_files:\n",
    "    raise FileNotFoundError(\"No cleaned CSV found. Run Notebook 03 first.\")\n",
    "\n",
    "latest_cleaned = max(cleaned_files, key=os.path.getmtime)\n",
    "print(\" Using cleaned dataset:\", latest_cleaned)\n",
    "\n",
    "df = pd.read_csv(latest_cleaned)\n",
    "print(\"Shape:\", df.shape)\n",
    "\n",
    "# Detect target column\n",
    "\n",
    "possible_targets = [c for c in df.columns if c.lower() in [\"target\", \"label\", \"class\", \"species\", \"outcome\"]]\n",
    "\n",
    "if len(possible_targets) > 0:\n",
    "    target = possible_targets[0]\n",
    "else:\n",
    "    # fallback: assume last column is target\n",
    "    target = df.columns[-1]\n",
    "\n",
    "print(\" Target column detected:\", target)\n",
    "\n",
    "#type of model\n",
    "if df[target].dtype in ['int64', 'float64']:\n",
    "    task_type = \"regression\"\n",
    "elif df[target].dtype == 'object' or df[target].dtype.name == 'category':\n",
    "    task_type = \"classification\"\n",
    "else:\n",
    "    # numeric but only few unique values => classification\n",
    "    if df[target].nunique() < 20:\n",
    "        task_type = \"classification\"\n",
    "    else:\n",
    "        task_type = \"regression\"\n",
    "\n",
    "print(\" Task type detected:\", task_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03060685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.3.2 in c:\\users\\chala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\chala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn==1.3.2) (1.25.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\chala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn==1.3.2) (3.6.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\chala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn==1.3.2) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\chala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn==1.3.2) (1.5.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn==1.3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d85642d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn import successful!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print(\"sklearn import successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9820eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (120, 4) (120,)\n",
      "Testing shape: (30, 4) (30,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Train/test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y  # stratify keeps class balance\n",
    ")\n",
    "\n",
    "print(\"Training shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing shape:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f643da23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Models available for training:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'LogisticRegression': LogisticRegression(max_iter=1000),\n",
       " 'RandomForest': RandomForestClassifier(n_estimators=200),\n",
       " 'KNN': KNeighborsClassifier(),\n",
       " 'SVC': SVC(probability=True),\n",
       " 'XGBoost': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "               colsample_bylevel=None, colsample_bynode=None,\n",
       "               colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "               enable_categorical=False, eval_metric='mlogloss',\n",
       "               feature_types=None, feature_weights=None, gamma=None,\n",
       "               grow_policy=None, importance_type=None,\n",
       "               interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "               max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "               max_delta_step=None, max_depth=4, max_leaves=None,\n",
       "               min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "               multi_strategy=None, n_estimators=200, n_jobs=None,\n",
       "               num_parallel_tree=None, ...)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except:\n",
    "    xgb_available = False\n",
    "    print(\" XGBoost not installed. Skipping XGBClassifier.\")\n",
    "\n",
    "# Model dictionary\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=200),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"SVC\": SVC(probability=True)\n",
    "}\n",
    "\n",
    "if xgb_available:\n",
    "    models[\"XGBoost\"] = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=4,\n",
    "        eval_metric=\"mlogloss\"\n",
    "    )\n",
    "\n",
    "print(\" Models available for training:\")\n",
    "models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e868049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\chala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\chala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from xgboost) (1.25.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\chala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from xgboost) (1.11.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b875a06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training LogisticRegression...\n",
      "    Accuracy: 0.9667\n",
      " Training RandomForest...\n",
      "    Accuracy: 0.9333\n",
      " Training KNN...\n",
      "    Accuracy: 1.0000\n",
      " Training SVC...\n",
      "    Accuracy: 0.9667\n",
      " Training XGBoost...\n",
      "    Accuracy: 0.9333\n",
      "\n",
      " Model performance summary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'LogisticRegression': 0.9666666666666667,\n",
       " 'RandomForest': 0.9333333333333333,\n",
       " 'KNN': 1.0,\n",
       " 'SVC': 0.9666666666666667,\n",
       " 'XGBoost': 0.9333333333333333}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "results = {}           # model -> accuracy\n",
    "trained_models = {}    # store fitted models\n",
    "\n",
    "# Create label encoder for XGBoost only , since it needs 0,1,2 as inputs \n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\" Training {name}...\")\n",
    "\n",
    "    # XGBoost needs encoded labels\n",
    "    if \"XGBoost\" in name:\n",
    "        model.fit(X_train, y_train_enc)\n",
    "        y_pred = model.predict(X_test)\n",
    "        # Evaluate using encoded labels\n",
    "        acc = accuracy_score(y_test_enc, y_pred)\n",
    "    else:\n",
    "        # Other models handle string labels fine\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    results[name] = acc\n",
    "    trained_models[name] = model\n",
    "\n",
    "    print(f\"    Accuracy: {acc:.4f}\")\n",
    "\n",
    "print(\"\\n Model performance summary:\")\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7abc547c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Model: KNN (Accuracy: 1.0000)\n",
      " Model saved at:\n",
      "../results/models/KNN_20251202_223130.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "# Finding best model based on accuracy\n",
    "best_model_name = max(results, key=results.get)\n",
    "best_model = trained_models[best_model_name]\n",
    "best_score = results[best_model_name]\n",
    "\n",
    "print(f\" Best Model: {best_model_name} (Accuracy: {best_score:.4f})\")\n",
    "\n",
    "# Create timestamped filename\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_path = f\"../results/models/{best_model_name}_{timestamp}.pkl\"\n",
    "\n",
    "# Saving as pkl to later load in phase 2\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "print(\" Model saved at:\")\n",
    "print(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbc5dc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training summary saved at:\n",
      "../results/logs/training_summary_20251202_223130.json\n"
     ]
    }
   ],
   "source": [
    "# overall report for all models :-\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "training_summary = {\n",
    "    \"task_type\": task_type,\n",
    "    \"target_column\": target,\n",
    "    \"model_performance\": results,\n",
    "    \"best_model\": best_model_name,\n",
    "    \"best_score\": best_score,\n",
    "}\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_path = f\"../results/logs/training_summary_{timestamp}.json\"\n",
    "\n",
    "# Save log\n",
    "with open(log_path, \"w\") as f:\n",
    "    json.dump(training_summary, f, indent=4)\n",
    "\n",
    "print(\" Training summary saved at:\")\n",
    "print(log_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c90deb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Feature importance extracted for: RandomForest\n",
      " Feature importance extracted for: XGBoost\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RandomForest': {'SepalLengthCm': 0.12426685779780329,\n",
       "  'SepalWidthCm': 0.01867383268253318,\n",
       "  'PetalLengthCm': 0.4443348638796532,\n",
       "  'PetalWidthCm': 0.4127244456400104},\n",
       " 'XGBoost': {'SepalLengthCm': 0.021314733,\n",
       "  'SepalWidthCm': 0.022769462,\n",
       "  'PetalLengthCm': 0.519456,\n",
       "  'PetalWidthCm': 0.43645984}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "feature_importance = {}\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    try:\n",
    "        # RandomForest, XGBoost\n",
    "        if hasattr(model, \"feature_importances_\"):\n",
    "            importances = model.feature_importances_\n",
    "            feature_importance[name] = dict(zip(X.columns, importances))\n",
    "            print(f\" Feature importance extracted for: {name}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# If no model supports feature importance\n",
    "if len(feature_importance) == 0:\n",
    "    print(\" No models with feature importance available.\")\n",
    "\n",
    "feature_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84bcfe9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Feature importance saved at:\n",
      "../results/models/feature_importance_20251202_223130.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# Convert all numpy.float32 â†’ python float\n",
    "fi_cleaned = {}\n",
    "\n",
    "for model_name, fi_dict in feature_importance.items():\n",
    "    fi_cleaned[model_name] = {k: float(v) for k, v in fi_dict.items()}\n",
    "\n",
    "# Save JSON\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "fi_path = f\"../results/models/feature_importance_{timestamp}.json\"\n",
    "\n",
    "with open(fi_path, \"w\") as f:\n",
    "    json.dump(fi_cleaned, f, indent=4)\n",
    "\n",
    "print(\" Feature importance saved at:\")\n",
    "print(fi_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae4b50d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
